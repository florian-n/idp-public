{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# os.environ['JAX_PLATFORM_NAME'] = \"cpu\"\n",
    "# os.environ['JAX_PLATFORMS'] = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 22:14:14.559220: W external/xla/xla/service/gpu/nvptx_compiler.cc:765] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.5.82). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "/home/florian/anaconda3/envs/jaxidp/lib/python3.11/site-packages/google/colab/data_table.py:30: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils import traitlets as _traitlets\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax import lax\n",
    "\n",
    "import numpy as onp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from simulation.simulate_full import run_entire_simulation\n",
    "\n",
    "from analyzers import defaultvalues as dv, database, loss as loss_anaylzer, gradients as grad_analyzer\n",
    "\n",
    "database.set_filename(\"../data/grad_analyzer/nsimulations_scan.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "N_GRADIENTS = 4\n",
    "nsimulations_values = onp.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print(nsimulations_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(grads):\n",
    "    amean, astd = grad_analyzer.analyze_gradients_absolute(grads)\n",
    "    mmean, mstd = grad_analyzer.analyze_gradients_magnitudal(grads)\n",
    "\n",
    "    print(f\"Absolute mean: {amean}, Absolute std: {astd}\")\n",
    "    print(f\"Magnitudal mean: {mmean}, Magnitudal std: {mstd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 1, was already computed\n",
      "Skipping 2, was already computed\n",
      "Skipping 3, was already computed\n",
      "Skipping 4, was already computed\n",
      "Skipping 5, was already computed\n",
      "Skipping 6, was already computed\n",
      "Skipping 7, was already computed\n",
      "==== for 8 simulations ====\n"
     ]
    }
   ],
   "source": [
    "for nsims in nsimulations_values:\n",
    "    existing_keys = database.get_existing_keys()\n",
    "    if nsims in existing_keys:\n",
    "        print(f\"Skipping {nsims}, was already computed\")\n",
    "        continue\n",
    "\n",
    "    grads = []\n",
    "    print(f\"==== for {nsims} simulations ====\")\n",
    "    for i in range(1, N_GRADIENTS + 1):\n",
    "        keys = jnp.array([random.randrange(0, 20000) for _ in range(nsims)])\n",
    "        def simulation_wrapper(LJ_SIGMA_OO: float, key: int) -> float:\n",
    "            prediction = run_entire_simulation(\n",
    "                LJ_SIGMA_OO, \n",
    "                dv.N_STEPS, \n",
    "                dv.N_MOLECULES_PER_AXIS, \n",
    "                dv.N_SNAPSHOTS, \n",
    "                dv.N_Q, \n",
    "                key)\n",
    "            reference = run_entire_simulation(\n",
    "                dv.LJ_SIGMA_OO, \n",
    "                dv.N_STEPS, \n",
    "                dv.N_MOLECULES_PER_AXIS, \n",
    "                dv.N_SNAPSHOTS, \n",
    "                dv.N_Q, \n",
    "                key)\n",
    "            return loss_anaylzer.L1_loss(prediction, reference)\n",
    "        v_simulation_wrapper = jax.vmap(simulation_wrapper, in_axes=(None, 0))\n",
    "        def multiple_simulation_wrapper(LJ_SIGMA_OO: float, keys) -> float:\n",
    "            losses = v_simulation_wrapper(LJ_SIGMA_OO, keys)\n",
    "            return jnp.mean(losses)\n",
    "\n",
    "        grad_fn = jax.grad(multiple_simulation_wrapper, 0)\n",
    "        grad = grad_fn(3.1, keys)\n",
    "        print(grad)\n",
    "        grads.append(grad)\n",
    "\n",
    "    grads = jnp.array(grads)\n",
    "    print_info(grads)\n",
    "    database.save_intermediate_result(nsims, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 1, was already computed\n",
      "==== for 2 simulations ====\n",
      "1.2006835602501748e+49\n"
     ]
    }
   ],
   "source": [
    "database.set_filename(\"../data/grad_analyzer/nsimulations_scan2.npz\")\n",
    "for nsims in nsimulations_values:\n",
    "    existing_keys = database.get_existing_keys()\n",
    "    if nsims in existing_keys:\n",
    "        print(f\"Skipping {nsims}, was already computed\")\n",
    "        continue\n",
    "\n",
    "    grads = []\n",
    "    print(f\"==== for {nsims} simulations ====\")\n",
    "    for i in range(1, N_GRADIENTS + 1):\n",
    "        keys = jnp.array([random.randrange(0, 20000) for _ in range(nsims)])\n",
    "        def simulation_wrapper(LJ_SIGMA_OO: float, key: int) -> float:\n",
    "            prediction = run_entire_simulation(\n",
    "                LJ_SIGMA_OO, \n",
    "                dv.N_STEPS, \n",
    "                dv.N_MOLECULES_PER_AXIS, \n",
    "                dv.N_SNAPSHOTS, \n",
    "                dv.N_Q, \n",
    "                key)\n",
    "            reference = run_entire_simulation(\n",
    "                dv.LJ_SIGMA_OO, \n",
    "                dv.N_STEPS, \n",
    "                dv.N_MOLECULES_PER_AXIS, \n",
    "                dv.N_SNAPSHOTS, \n",
    "                dv.N_Q, \n",
    "                key)\n",
    "            return loss_anaylzer.L1_loss(prediction, reference)\n",
    "        def multiple_simulation_wrapper(LJ_SIGMA_OO: float) -> float:\n",
    "            _, losses = lax.scan(lambda _, xscan: (0, simulation_wrapper(LJ_SIGMA_OO, xscan)), 0, keys)            \n",
    "            return jnp.mean(losses)\n",
    "\n",
    "        grad_fn = jax.grad(multiple_simulation_wrapper)\n",
    "        grad = grad_fn(3.1)\n",
    "        print(grad)\n",
    "        grads.append(grad)\n",
    "\n",
    "    grads = onp.array(grads)\n",
    "    print_info(grads)\n",
    "    database.save_intermediate_result(nsims, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = database.load_result()\n",
    "for keys, grads in zip(keys, values):\n",
    "    print(f\"==== for {keys//500}ps ====\")\n",
    "    print_info(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jaxidp)",
   "language": "python",
   "name": "jaxidp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
